# Copyright 2017 The TensorFlow Authors. All Rights Reserved.# Licensed under the Apache License, Version 2.0 (the "License");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at#     http://www.apache.org/licenses/LICENSE-2.0# Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# ==============================================================================r"""Benchmarks for low-level eager execution primitives.To run CPU benchmarks:bazel run -c opt benchmarks_test -- --.To run GPU benchmarks:bazel run --cuda -c opt --"-mavx" benchmarks_test -- \--.To run a subset of benchmarks using --benchmarks flag.--benchmarks: the list of benchmarks to run. The specified value is interpretedas a regular expression and any benchmark whose name contains a partial matchto the regular expression is executed.e.g. --".*matmul*." will run all matmul related benchmarks."""from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionimport timeimport numpy as npimport sixfrom six.moves import xrange  # pylint: redefined-builtinfrom tensorflow.python import pywrap_tfefrom tensorflow.python.eager import backprop  # pylint: unused-importfrom tensorflow.python.eager import benchmarks_test_basefrom tensorflow.python.eager import contextfrom tensorflow.python.eager import corefrom tensorflow.python.eager import def_functionfrom tensorflow.python.eager import forwardpropfrom tensorflow.python.eager import functionfrom tensorflow.python.eager import testfrom tensorflow.python.framework import constant_opfrom tensorflow.python.framework import dtypesfrom tensorflow.python.framework import opsfrom tensorflow.python.framework import tensor_shapefrom tensorflow.python.framework import tensor_specfrom tensorflow.python.framework import test_utilfrom tensorflow.python.ops import array_opsfrom tensorflow.python.ops import control_flow_opsfrom tensorflow.python.ops import functional_opsfrom tensorflow.python.ops import gen_array_opsfrom tensorflow.python.ops import gen_math_opsfrom tensorflow.python.ops import math_opsfrom tensorflow.python.ops import nn_opsfrom tensorflow.python.ops import random_opsfrom tensorflow.python.ops import resource_variable_opsfrom tensorflow.python.util import nestfrom tensorflow.python.util import tf_inspect"/device:CPU:0""/device:GPU:0"Nonedef c_tfe_py_fastpath_execute(a,False,False,None):context.context()assert ctx.executing_eagerly(), "The prototype doesn't contain C code for graph construction"try:return pywrap_tfe.TFE_Py_FastPathExecute(ctx,"MatMul", name,a, b, "transpose_a", transpose_a,"transpose_b", transpose_b)except core._NotOkStatusException as e:if name is not None:e.message + " name: " + nameelse:e.messagesix.raise_from(core._status_to_exception(e.code, message), None)def run_benchmark(func, num_iters, None):context.context()with context.execution_mode(execution_mode):# call func to warm upfunc()if = context.ASYNC:ctx.executor.wait()time.time()for _ in xrange(num_iters):func()if = context.ASYNC:ctx.executor.wait()time.time()return end - startclass MicroBenchmarks(benchmarks_test_base.MicroBenchmarksBase):def __init__(self):# used for multiply benchmarksself. random_ops.random_uniform([2])# used for matmul benchmarksself. random_ops.random_uniform((2, 2))self. random_ops.random_uniform((100, 784))self. 30000self. 30000# used for conv2d benchmarksself. random_ops.random_uniform((8, 28, 28, 3))self. random_ops.random_uniform((1, 3, 3, 1))def _get_benchmark_name(self):"""Mostly copied from benchmark.py _get_name()."""tf_inspect.stack()Nonefor frame in stack[::-1]:frame[0].f_localsf_locals.get("self", None)if isinstance(f_self, test.Benchmark):frame[3]  # Get the method name# This is a hack to get around the fact that some methods might have a# disable_tfrt decorator around them. In that case a function called# 'decorated' wraps the real called function underneath and so we# peek one deeper into the stack to get the real name.if = "decorated":continueelse:breakif name is None:raise ValueError("Unable to determine calling Benchmark function.")if context.is_tfrt_enabled():name + "_tfrt"return namedef _run(self, func, num_iters, None):self.run_report(run_benchmark, func, num_iters, execution_mode)def benchmark_create_np_array(self):lambda: np.array([3.0])self._run(func, 30000)def _benchmark_create_tensor(self, value, dtype, device):"""Benchmark overheads of creating a Tensor object."""if = GPU:# Warmup the GPUops.EagerTensor(value, device)def func():ops.EagerTensor(value, device, dtype)self._run(func, 30000)def _benchmark_create_constant(self, value, dtype, True):global GLOBAL_TEST_VALUEvaluedef cached_func():constant_op.constant(value, dtype)def uncached_func():global GLOBAL_TEST_VALUEGLOBAL_TEST_VALUE += 1constant_op.constant(GLOBAL_TEST_VALUE, dtype)cached_func if cached else uncached_funcwith ops.device("GPU:0" if context.num_gpus() else "CPU:0"):for _ in range(1000):func()  # Warmup.self._run(func, 3000)def benchmark_create_float_constant(self):self._benchmark_create_constant(42.0, None)def benchmark_create_float_constant_uncached(self):self._benchmark_create_constant(42.0, None, False)def benchmark_create_int32_constant(self):if context.num_gpus():return  # int32 constants are always allocated on CPU.self._benchmark_create_constant(42, dtypes.int32)def benchmark_create_int32_constant_uncached(self):if context.num_gpus():return  # int32 constants are always allocated on CPU.self._benchmark_create_constant(42, dtypes.int32, False)def _benchmark_add(self, a, b):def func():return memoryview(math_ops.add_v2(a, b))with ops.device("GPU:0" if context.num_gpus() else "CPU:0"):for _ in range(1000):func()  # Warmup.self._run(func, 30000)def _benchmark_add_operator_overload(self, a, b):def func():return memoryview(a + b)with ops.device("GPU:0" if context.num_gpus() else "CPU:0"):for _ in range(1000):func()  # Warmup.self._run(func, 30000)def benchmark_add_float_scalars(self):self._benchmark_add(42.0, 24.0)def benchmark_add_int32_scalars(self):self._benchmark_add(42, 24)def benchmark_add_float_scalar_tensor(self):constant_op.constant(42.0)constant_op.constant(24.0)self._benchmark_add(tensor_a, tensor_b)def benchmark_add_float_scalar_tensor_overloaded_operator(self):constant_op.constant(42.0)constant_op.constant(24.0)self._benchmark_add_operator_overload(tensor_a, tensor_b)def benchmark_add_int32_scalar_tensor(self):constant_op.constant(42)constant_op.constant(24)self._benchmark_add(tensor_a, tensor_b)def benchmark_add_float_dense_tensor(self):constant_op.constant([[42.0, 42.0], [42.0, 42.0]])constant_op.constant([[24.0, 24.0], [24.0, 24.0]])self._benchmark_add(tensor_a, tensor_b)def benchmark_add_int32_dense_tensor(self):constant_op.constant([[42, 42], [42, 42]])constant_op.constant([[24, 24], [24, 24]])self._benchmark_add(tensor_a, tensor_b)def benchmark_create_float_tensor_from_list_CPU(self):self._benchmark_create_tensor([[3.0]], dtypes.float32.as_datatype_enum, CPU)def benchmark_create_float_tensor_from_np_array_CPU(self):self._benchmark_create_tensor(np.array([[3.0]], np.float32), dtypes.float32.as_datatype_enum,CPU)def benchmark_create_int32_tensor_from_list_CPU(self):self._benchmark_create_tensor([[3]], dtypes.int32.as_datatype_enum, CPU)def benchmark_create_int32_tensor_from_np_array_CPU(self):self._benchmark_create_tensor(np.array([[3]], np.int32), dtypes.int32.as_datatype_enum, CPU)def benchmark_create_float_tensor_from_list_GPU(self):if not context.num_gpus():returnself._benchmark_create_tensor([[3.0]], dtypes.float32.as_datatype_enum, GPU)def benchmark_create_float_tensor_from_np_array_GPU(self):if not context.num_gpus():returnself._benchmark_create_tensor(np.array([[3.0]], np.float32), dtypes.float32.as_datatype_enum,GPU)def benchmark_create_int32_tensor_from_list_GPU(self):# int32's are kept on host memory even when executing on GPU.if not context.num_gpus():returnself._benchmark_create_tensor([[3]], dtypes.int32.as_datatype_enum, GPU)def benchmark_create_int32_tensor_from_np_array_GPU(self):# int32's are kept on host memory even when executing on GPU.if not context.num_gpus():returnself._benchmark_create_tensor(np.array([[3]], np.int32), dtypes.int32.as_datatype_enum, GPU)def benchmark_index_tensor_with_literal(self):lambda: constant_op.constant([3.0])[0]self._run(func, 30000)def benchmark_index_tensor_with_tensor(self):lambda constant_op.constant(0): constant_op.constant([3.0])[idx]self._run(func, 30000)def benchmark_index_tensor_with_np_array(self):lambda np.array(0): constant_op.constant([3.0])[idx]self._run(func, 30000)def _benchmark_np_multiply(self, m, num_iters):m.cpu().numpy()lambda: a * aself._run(func, num_iters)def _benchmark_tf_multiply(self, m, num_iters):lambda: m * mself._run(func, num_iters)def _benchmark_tf_conv2d(self, m1, m2, num_iters):lambda: nn_ops.conv2d(m1, m2, [1, 1, 1, 1], "VALID")self._run(func, num_iters)def _benchmark_tf_multiply_op(self, m, num_iters):lambda: math_ops.multiply(m, m)self._run(func, num_iters)def benchmark_np_multiply(self):self._benchmark_np_multiply(self._m_2, 30000)def benchmark_tf_multiply_CPU(self):with context.device(CPU):self._m_2.cpu()self._benchmark_tf_multiply(m, 30000)def benchmark_tf_multiply_GPU(self):if not context.num_gpus():returnwith context.device(GPU):self._m_2.gpu()self._benchmark_tf_multiply(m, 30000)def benchmark_tf_multiply_op_CPU(self):with context.device(CPU):self._m_2.cpu()self._benchmark_tf_multiply_op(m, 30000)def benchmark_tf_multiply_op_GPU(self):if not context.num_gpus():returnwith context.device(GPU):self._m_2.gpu()self._benchmark_tf_multiply_op(m, 30000)def benchmark_tf_conv2d_CPU(self):with context.device(CPU):self._m_8_28_28_3.cpu()self._m_1_3_3_1.cpu()self._benchmark_tf_conv2d(m1, m2, 30000)def benchmark_tf_conv2d_GPU(self):if not context.num_gpus():returnwith context.device(GPU):self._m_8_28_28_3.gpu()self._m_1_3_3_1.gpu()self._benchmark_tf_conv2d(m1, m2, 30000)def benchmark_tf_identity(self):self._m_2self._run(lambda: gen_array_ops.identity(m), 30000)def benchmark_slowpath_tf_identity(self):self._run(lambda: gen_array_ops.identity(1), 30000)def benchmark_tfe_py_execute_identity(self):self._m_2context.context()._handle("T", self._m_2.dtype.as_datatype_enum)[m]def f():pywrap_tfe.TFE_Py_Execute(ctx_handle, None, "Identity", inputs, attrs, 1)self._run(f, 30000)def benchmark_tf_gradient_function_identity(self):with context.device(CPU):gen_array_ops.identity(self._m_2)self._run(lambda: backprop.gradients_function(gen_array_ops.identity, [0])(m),30000)def benchmark_tf_gradient_forward_identity(self):with backprop.GradientTape() as tape:self._m_2tape.watch(m)self._run(lambda: gen_array_ops.identity(m), 30000)def benchmark_tf_gradient_tape_push_pop(self):def f():with backprop.GradientTape():passself._run(f, 30000)def benchmark_tf_gradient_function_no_op(self):with context.device(CPU):gen_array_ops.identity(self._m_2)self._run(lambda: backprop.gradients_function(lambda x: x, [0])(m), 30000)def _benchmark_np_matmul(self, m, transpose_b, num_iters):m.cpu().numpy()a.T if transpose_b else alambda: np.dot(a, b)self._run(func, num_iters)def _benchmark_tf_matmul(self, m, transpose_b, num_iters,None):lambda: math_ops.matmul(m, m, transpose_b)self._run(func, num_iters, execution_mode)def _benchmark_gen_math_ops_matmul(self, m, transpose_b, num_iters):def func():gen_math_ops.mat_mul(m, m, transpose_b)self._run(func, num_iters)def _benchmark_tfe_py_fastpath_execute_matmul(self, m, transpose_b,num_iters):def func():c_tfe_py_fastpath_execute(m, m, transpose_b)self._run(func, num_iters)def _benchmark_tfe_py_execute_matmul(self, m, transpose_b, num_iters):[m, m]# pylint: protected-accesscontext.context()._handle# pylint: protected-accesscontext.context().device_name("transpose_a", False, "transpose_b", transpose_b, "T",m.dtype.as_datatype_enum)def func():pywrap_tfe.TFE_Py_Execute(ctx_handle, device, "MatMul", inputs, attrs, 1)self._run(func, num_iters)def _benchmark_defun_matmul(self,transpose_b,num_iters,None):function.defun(math_ops.matmul)lambda: f(m, m, transpose_b)self._run(func, num_iters, execution_mode)def _benchmark_defun_matmul_with_signature(self,num_iters,None):@def_function.function([tensor_spec.TensorSpec([2, 2], dtypes.float32)])def defun_matmul(m):return math_ops.matmul(m, m)lambda: defun_matmul(m)self._run(func, num_iters, execution_mode)def _benchmark_defun_matmul_relaxed_shape(self,num_iters,None):@def_function.function(True)def defun_matmul(m):return math_ops.matmul(m, m)random_ops.random_uniform((3, 3))defun_matmul(m_3_by_3)lambda: defun_matmul(m)self._run(func, num_iters, execution_mode)def _benchmark_defun_args_matmul(self, m, num_iters, None):@def_function.functiondef defun_matmul(m):return math_ops.matmul(m, m)lambda: defun_matmul(m)self._run(func, num_iters, execution_mode)def _benchmark_nested_defun_matmul(self, m, transpose_b, num_iters):function.defun(math_ops.matmul)@function.defundef outer(a, b, c, transpose_b):return math_ops.matmul(inner(a, b, transpose_b), c)lambda: outer(m, m, m, transpose_b)# Warmup before benchmarkfor _ in range(1000):func()self._run(func, num_iters)def _benchmark_defun_matmul_forward_backward(self,transpose_b,num_iters,None):def_function.function(math_ops.matmul)def func():with backprop.GradientTape() as gt:gt.watch(m)f(m, m, transpose_b)gt.gradient(y, m)self._run(func, num_iters, execution_mode)def _benchmark_read_variable(self, m, num_iters):self._run(m.value, num_iters)def _benchmark_matmul_read_variable(self, m, num_iters):self._benchmark_gen_math_ops_matmul(m, False, num_iters)def _benchmark_matmul_read_variable_with_tape(self, m, num_iters):with backprop.GradientTape() as tape:tape.watch(m)self._benchmark_gen_math_ops_matmul(m, False, num_iters)def _benchmark_read_variable_with_tape(self, m, num_iters):with backprop.GradientTape() as tape:tape.watch(m)self._run(m.value, num_iters)# Benchmarks for A^2, A of dimension 2 by 2.def benchmark_np_matmul_2_by_2(self):self._benchmark_np_matmul(self._m_2_by_2, False, self._num_iters_2_by_2)def benchmark_tf_matmul_2_by_2_CPU(self):with context.device(CPU):self._m_2_by_2.cpu()self._benchmark_tf_matmul(m, False, self._num_iters_2_by_2)def benchmark_tf_matmul_2_by_2_CPU_async(self):with context.device(CPU):self._m_2_by_2.cpu()self._benchmark_tf_matmul(False,self._num_iters_2_by_2,context.ASYNC)def benchmark_gen_math_ops_matmul_2_by_2_CPU(self):with context.device(CPU):self._m_2_by_2.cpu()self._benchmark_gen_math_ops_matmul(m, False, self._num_iters_2_by_2)def benchmark_tfe_py_fastpath_execute_matmul_2_by_2_CPU(self):with context.device(CPU):self._m_2_by_2.cpu()self._benchmark_tfe_py_fastpath_execute_matmul(m, False, self._num_iters_2_by_2)def benchmark_tfe_py_execute_matmul_2_by_2_CPU(self):with context.device(CPU):self._m_2_by_2.cpu()self._benchmark_tfe_py_execute_matmul(m, False, self._num_iters_2_by_2)def benchmark_defun_matmul_2_by_2_CPU(self):with context.device(CPU):self._m_2_by_2.cpu()self._benchmark_defun_matmul(m, False, self._num_iters_2_by_2)def benchmark_defun_matmul_2_by_2_with_signature_CPU(self):with context.device(CPU):self._m_2_by_2.cpu()self._benchmark_defun_matmul_with_signature(m, self._num_iters_2_by_2)def benchmark_defun_matmul_2_by_2_relaxed_shape_CPU(self):with context.device(CPU):self._m_2_by_2.cpu()self._benchmark_defun_matmul_relaxed_shape(m, self._num_iters_2_by_2)def benchmark_defun_args_matmul_2_by_2_CPU(self):with context.device(CPU):self._m_2_by_2.cpu()self._benchmark_defun_args_matmul(m, self._num_iters_2_by_2)def benchmark_defun_matmul_2_by_2_CPU_async(self):with context.device(CPU):self._m_2_by_2.cpu()self._benchmark_defun_matmul(False,self._num_iters_2_by_2,context.ASYNC)def _benchmark_matmul_forward_backward_2_by_2_CPU(self, False):def_function.run_functions_eagerly(run_eager)with context.device(CPU):self._m_2_by_2.cpu()self._benchmark_defun_matmul_forward_backward(m, False, self._num_iters_2_by_2)def_function.run_functions_eagerly(False)def _benchmark_matmul_forward_backward_2_by_2_CPU_async(self, False):def_function.run_functions_eagerly(run_eager)with context.device(CPU):self._m_2_by_2.cpu()self._benchmark_defun_matmul_forward_backward(False,self._num_iters_2_by_2,context.ASYNC)def benchmark_defun_matmul_forward_backward_2_by_2_CPU(self):self._benchmark_matmul_forward_backward_2_by_2_CPU(False)def benchmark_defun_matmul_forward_backward_2_by_2_CPU_async(self):self._benchmark_matmul_forward_backward_2_by_2_CPU_async(False)def benchmark_defun_eager_matmul_forward_backward_2_by_2_CPU(self):self._benchmark_matmul_forward_backward_2_by_2_CPU(True)def benchmark_defun_eager_matmul_forward_backward_2_by_2_CPU_async(self):self._benchmark_matmul_forward_backward_2_by_2_CPU_async(True)def benchmark_tf_matmul_2_by_2_GPU(self):if not context.num_gpus():returnwith context.device(GPU):self._m_2_by_2.gpu()self._benchmark_tf_matmul(m, False, self._num_iters_2_by_2)def benchmark_tf_matmul_2_by_2_GPU_async(self):if not context.num_gpus():returnwith context.device(GPU):self._m_2_by_2.gpu()self._benchmark_tf_matmul(False,self._num_iters_2_by_2,context.ASYNC)def benchmark_gen_math_ops_matmul_2_by_2_GPU(self):if not context.num_gpus():returnwith context.device(GPU):self._m_2_by_2.gpu()self._benchmark_gen_math_ops_matmul(m, False, self._num_iters_2_by_2)def benchmark_tfe_py_execute_matmul_2_by_2_GPU(self):if not context.num_gpus():returnwith context.device(GPU):self._m_2_by_2.gpu()self._benchmark_tfe_py_execute_matmul(m, False, self._num_iters_2_by_2)def benchmark_defun_matmul_2_by_2_GPU(self):if not context.num_gpus():returnwith context.device(GPU):self._m_2_by_2.gpu()self._benchmark_defun_matmul(m, False, self._num_iters_2_by_2)def benchmark_defun_matmul_2_by_2_with_signature_GPU(self):if not context.num_gpus():returnwith context.device(GPU):self._m_2_by_2.gpu()self._benchmark_defun_matmul_with_signature(m, self._num_iters_2_by_2)def benchmark_defun_matmul_2_by_2_relaxed_shape_GPU(self):if not context.num_gpus():returnwith context.device(GPU):self._m_2_by_2.gpu()self._benchmark_defun_matmul_relaxed_shape(m, self._num_iters_2_by_2)def benchmark_defun_args_matmul_2_by_2_GPU(self):if not context.num_gpus():returnwith context.device(GPU):self._m_2_by_2.gpu()self._benchmark_defun_args_matmul(m, self._num_iters_2_by_2)def benchmark_defun_matmul_2_by_2_GPU_async(self):if not context.num_gpus():returnwith context.device(GPU):self._m_2_by_2.gpu()self._benchmark_defun_matmul(False,self._num_iters_2_by_2,context.ASYNC)def benchmark_nested_defun_matmul_2_by_2(self):self._m_2_by_2.cpu()self._benchmark_nested_defun_matmul(m, False, self._num_iters_2_by_2)# Benchmarks for AA.T, A of dimension 100 by 784.def benchmark_np_matmul_100_by_784(self):self._benchmark_np_matmul(self._m_100_by_784,True,self._num_iters_100_by_784)def benchmark_tf_matmul_100_by_784_CPU(self):with context.device(CPU):self._m_100_by_784.cpu()self._benchmark_tf_matmul(m, True, self._num_iters_100_by_784)def benchmark_tf_matmul_100_by_784_CPU_async(self):with context.device(CPU):self._m_100_by_784.cpu()self._benchmark_tf_matmul(True,self._num_iters_100_by_784,context.ASYNC)def benchmark_gen_math_ops_matmul_100_by_784_CPU(self):with context.device(CPU):self._m_100_by_784.cpu()self._benchmark_gen_math_ops_matmul(m, True, self._num_iters_100_by_784)def benchmark_tfe_py_fastpath_execute_matmul_100_by_784_CPU(self):with context.device(CPU):self._m_100_by_784.cpu()self._benchmark_tfe_py_fastpath_execute_matmul(m, True, self._num_iters_100_by_784)def benchmark_tfe_py_execute_matmul_100_by_784_CPU(self):with context.device(CPU):self._m_100_by_784.cpu()self._benchmark_tfe_py_execute_matmul(m, True, self._num_iters_100_by_784)def benchmark_defun_matmul_100_by_784_CPU(self):with context.device(CPU):self._m_100_by_784.cpu()self._benchmark_defun_matmul(m, True, self._num_iters_100_by_784)def benchmark_tf_matmul_100_by_784_GPU(self):if not context.num_gpus():returnwith context.device(GPU):self._m_100_by_784.gpu()self._benchmark_tf_matmul(m, True, self._num_iters_100_by_784)def benchmark_tf_matmul_100_by_784_GPU_async(self):if not context.num_gpus():returnwith context.device(GPU):self._m_100_by_784.gpu()self._benchmark_tf_matmul(True,self._num_iters_100_by_784,context.ASYNC)def benchmark_gen_math_ops_matmul_100_by_784_GPU(self):if not context.num_gpus():returnwith context.device(GPU):self._m_100_by_784.gpu()self._benchmark_gen_math_ops_matmul(m, True, self._num_iters_100_by_784)def benchmark_tfe_py_execute_matmul_100_by_784_GPU(self):if not context.num_gpus():returnwith context.device(GPU):self._m_100_by_784.gpu()self._benchmark_tfe_py_execute_matmul(m, True, self._num_iters_100_by_784)def benchmark_defun_matmul_100_by_784_GPU(self):if not context.num_gpus():returnwith context.device(GPU):self._m_100_by_784.gpu()self._benchmark_defun_matmul(m, True, self._num_iters_100_by_784)@test_util.disable_tfrt("b/169371527: Support inserting transfer op in lowering.")def benchmark_nested_defun_matmul_100_by_784_GPU(self):self._m_100_by_784.gpu()self._benchmark_nested_defun_matmul(m, True, self._num_iters_100_by_784)def _benchmark_forwardprop_matmul_CPU(self, shape):with ops.device(CPU):random_ops.random_uniform(shape).cpu()random_ops.random_uniform(shape).cpu()def func():with forwardprop.ForwardAccumulator(m, tangent) as acc:math_ops.matmul(m, m, True)return result, acc.jvp(result)# Warmup before benchmarkfor _ in range(100):func()self._run(func, 3000)def _benchmark_forwardprop_in_defun_matmul_CPU(self, shape):with ops.device(CPU):@def_function.functiondef compiled_function(x, tangent):with forwardprop.ForwardAccumulator(x, tangent) as acc:math_ops.matmul(x, x, True)return result, acc.jvp(result)random_ops.random_uniform(shape).cpu()random_ops.random_uniform(shape).cpu()lambda: compiled_function(m, tangent)# Warmup before benchmarkfor _ in range(100):func()self._run(func, 3000)def _benchmark_forwardprop_in_defun_of_defun_matmul_CPU(self, shape):with ops.device(CPU):def_function.function(math_ops.matmul)@def_function.function()def compiled_function(x, tangent):with forwardprop.ForwardAccumulator(x, tangent) as acc:matmul(x, x, True)return result, acc.jvp(result)random_ops.random_uniform(shape).cpu()random_ops.random_uniform(shape).cpu()lambda: compiled_function(m, tangent)# Warmup before benchmarkfor _ in range(100):func()self._run(func, 3000)def _benchmark_forwardprop_of_defun_matmul_CPU(self, shape):with ops.device(CPU):random_ops.random_uniform(shape).cpu()random_ops.random_uniform(shape).cpu()def_function.function(math_ops.matmul)def func():with forwardprop.ForwardAccumulator(m, tangent) as acc:matmul(m, m, True)return result, acc.jvp(result)# Warmup before benchmarkfor _ in range(100):func()self._run(func, 3000)def benchmark_forwardprop_matmul_256_by_2096_CPU(self):self._benchmark_forwardprop_matmul_CPU((256, 2096))def benchmark_forwardprop_in_defun_matmul_256_by_2096_CPU(self):self._benchmark_forwardprop_in_defun_matmul_CPU((256, 2096))def benchmark_forwardprop_in_defun_of_defun_matmul_256_by_2096_CPU(self):self._benchmark_forwardprop_in_defun_of_defun_matmul_CPU((256, 2096))def benchmark_forwardprop_of_defun_matmul_256_by_2096_CPU(self):self._benchmark_forwardprop_of_defun_matmul_CPU((256, 2096))def benchmark_forwardprop_matmul_100_by_784_CPU(self):self._benchmark_forwardprop_matmul_CPU((100, 784))def benchmark_forwardprop_in_defun_matmul_100_by_784_CPU(self):self._benchmark_forwardprop_in_defun_matmul_CPU((100, 784))def benchmark_forwardprop_in_defun_of_defun_matmul_100_by_784_CPU(self):self._benchmark_forwardprop_in_defun_of_defun_matmul_CPU((100, 784))def benchmark_forwardprop_of_defun_matmul_100_by_784_CPU(self):self._benchmark_forwardprop_of_defun_matmul_CPU((100, 784))def _benchmark_tf_reduce_logsumexp(self,CPU,None,False,False):with context.device(device):constant_op.constant([[1, 0.], [0., 0.]])if defunc:def_function.function(math_ops.reduce_logsumexp, xla_compile)lambda: reduce_func(x)else:lambda: math_ops.reduce_logsumexp(x)self._run(func, 3000, execution_mode)def benchmark_tf_reduce_logsumexp_CPU(self):self._benchmark_tf_reduce_logsumexp()def benchmark_tf_reduce_logsumexp_CPU_async(self):self._benchmark_tf_reduce_logsumexp(context.ASYNC)def benchmark_tf_reduce_logsumexp_GPU(self):self._benchmark_tf_reduce_logsumexp(GPU)def benchmark_tf_reduce_logsumexp_GPU_async(self):self._benchmark_tf_reduce_logsumexp(GPU,context.ASYNC)@test_util.disable_tfrt("b/169371527: Support inserting transfer op in lowering.")def benchmark_tf_reduce_logsumexp_CPU_defunc(self):self._benchmark_tf_reduce_logsumexp(True)@test_util.disable_tfrt("b/169371527: Support inserting transfer op in lowering.")def benchmark_tf_reduce_logsumexp_CPU_async_defun(self):self._benchmark_tf_reduce_logsumexp(context.ASYNC, True)def benchmark_tf_reduce_logsumexp_GPU_defun(self):self._benchmark_tf_reduce_logsumexp(GPU, True)def benchmark_tf_reduce_logsumexp_GPU_async_defun(self):self._benchmark_tf_reduce_logsumexp(GPU, context.ASYNC, True)def benchmark_tf_reduce_logsumexp_GPU_defun_compile(self):self._benchmark_tf_reduce_logsumexp(GPU, True, True)def benchmark_tf_reduce_logsumexp_GPU_async_defun_compile(self):self._benchmark_tf_reduce_logsumexp(GPU, context.ASYNC, True, True)def _benchmark_tf_tensordot(self, CPU, None):with context.device(device):array_ops.ones((2, 2))array_ops.ones((2, 2))lambda: math_ops.tensordot(a, b, [[1], [0]])self._run(func, 30000, execution_mode)def benchmark_tf_tensordot_CPU(self):self._benchmark_tf_tensordot()def benchmark_tf_tensordot_CPU_async(self):self._benchmark_tf_tensordot(context.ASYNC)def benchmark_tf_tensordot_GPU(self):self._benchmark_tf_tensordot(GPU)def benchmark_tf_tensordot_GPU_async(self):self._benchmark_tf_tensordot(GPU, context.ASYNC)def _benchmark_tf_zeros(self, shape, dtype, CPU):with context.device(device):lambda: array_ops.zeros(shape, dtype)self._run(func, 3000)def benchmark_tf_zeros_2_by_2_float32_CPU(self):self._benchmark_tf_zeros((2, 2), dtypes.float32)def benchmark_tf_zeros_2_by_2_bool_CPU(self):self._benchmark_tf_zeros((2, 2), dtypes.bool)def benchmark_tf_zeros_2_by_2_string_CPU(self):self._benchmark_tf_zeros((2, 2), dtypes.string)def benchmark_tf_zeros_2_by_2_float32_GPU(self):self._benchmark_tf_zeros((2, 2), dtypes.float32, GPU)def benchmark_tf_zeros_2_by_2_bool_GPU(self):self._benchmark_tf_zeros((2, 2), dtypes.bool, GPU)def benchmark_tf_zeros_30_by_30_float32_CPU(self):self._benchmark_tf_zeros((30, 30), dtypes.float32)def benchmark_tf_zeros_30_by_30_bool_CPU(self):self._benchmark_tf_zeros((30, 30), dtypes.bool)def benchmark_tf_zeros_30_by_30_string_CPU(self):self._benchmark_tf_zeros((30, 30), dtypes.string)def benchmark_tf_zeros_30_by_30_float32_GPU(self):self._benchmark_tf_zeros((30, 30), dtypes.float32, GPU)def benchmark_tf_zeros_30_by_30_bool_GPU(self):self._benchmark_tf_zeros((30, 30), dtypes.bool, GPU)def benchmark_tf_zeros_100_by_100_float32_CPU(self):self._benchmark_tf_zeros((100, 100), dtypes.float32)def benchmark_tf_zeros_100_by_100_bool_CPU(self):self._benchmark_tf_zeros((100, 100), dtypes.bool)def benchmark_tf_zeros_100_by_100_string_CPU(self):self._benchmark_tf_zeros((100, 100), dtypes.string)def benchmark_tf_zeros_100_by_100_float32_GPU(self):self._benchmark_tf_zeros((100, 100), dtypes.float32, GPU)def benchmark_tf_zeros_100_by_100_bool_GPU(self):self._benchmark_tf_zeros((100, 100), dtypes.bool, GPU)def _benchmark_tf_zeros_like(self, m, CPU):with context.device(device):lambda: array_ops.zeros_like(m)self._run(func, 3000)def benchmark_tf_zeros_like_CPU(self):self._benchmark_tf_zeros_like(self._m_2_by_2)def benchmark_tf_zeros_like_GPU(self):self._benchmark_tf_zeros_like(self._m_2_by_2, GPU)def benchmark_tf_zeros_like_variable_CPU(self):resource_variable_ops.ResourceVariable(self._m_2_by_2)self._benchmark_tf_zeros_like(m)def benchmark_tf_zeros_like_variable_GPU(self):resource_variable_ops.ResourceVariable(self._m_2_by_2)self._benchmark_tf_zeros_like(m, GPU)def _benchmark_tf_random_uniform_2_by_2(self,(2, 2),dtypes.int32,CPU):with context.device(device):def func():return random_ops.random_uniform(shape, 3, dtype)self._run(func, self._num_iters_2_by_2)def benchmark_tf_random_uniform_2_by_2_integer_CPU(self):self._benchmark_tf_random_uniform_2_by_2()def benchmark_tf_random_uniform_2_by_2_integer_GPU(self):self._benchmark_tf_random_uniform_2_by_2(GPU)def benchmark_tf_random_uniform_2_by_2_float_CPU(self):self._benchmark_tf_random_uniform_2_by_2(dtypes.float32)def benchmark_tf_random_uniform_2_by_2_float_GPU(self):self._benchmark_tf_random_uniform_2_by_2(dtypes.float32, GPU)def benchmark_tf_random_uniform_2_by_2_default_setting_CPU(self):with context.device(CPU):lambda: random_ops.random_uniform((2, 2))self._run(func, self._num_iters_2_by_2)def benchmark_tf_random_uniform_2_by_2_default_setting_GPU(self):with context.device(GPU):lambda: random_ops.random_uniform((2, 2))self._run(func, self._num_iters_2_by_2)def _benchmark_tf_dropout_2_by_2(self,0.5,True,None,CPU):if is_rate_tensor:constant_op.constant(rate, dtypes.float32)with context.device(device):def func():return nn_ops.dropout(self._m_2_by_2, rate, noise_shape)self._run(func, self._num_iters_2_by_2)def benchmark_tf_dropout_scalar_rate_2_by_2_CPU(self):self._benchmark_tf_dropout_2_by_2(False)def benchmark_tf_dropout_scalar_rate_2_by_2_GPU(self):self._benchmark_tf_dropout_2_by_2(False, GPU)def benchmark_tf_dropout_2_by_2_CPU(self):self._benchmark_tf_dropout_2_by_2()def benchmark_tf_dropout_2_by_2_GPU(self):self._benchmark_tf_dropout_2_by_2(GPU)def benchmark_tf_dropout_scalar_rate_2_by_2_CPU_rate_0(self):self._benchmark_tf_dropout_2_by_2(0, False)def benchmark_tf_dropout_scalar_rate_2_by_2_GPU_rate_0(self):self._benchmark_tf_dropout_2_by_2(0.0,False, GPU)def benchmark_tf_dropout_2_by_2_CPU_rate_0(self):self._benchmark_tf_dropout_2_by_2(0.0)def benchmark_tf_dropout_2_by_2_GPU_rate_0(self):self._benchmark_tf_dropout_2_by_2(0, GPU)def _benchmark_transpose(self,num_iters,None,False,None):lambda: array_ops.transpose(m, perm, conjugate)self._run(func, num_iters, execution_mode)def benchmark_tf_transpose_2_by_2_CPU(self):with context.device(CPU):self._m_2_by_2.cpu()self._benchmark_transpose(m, self._num_iters_2_by_2)def benchmark_tf_transpose_2_by_2_GPU(self):with context.device(GPU):self._m_2_by_2.gpu()self._benchmark_transpose(m, self._num_iters_2_by_2)def benchmark_tf_transpose_variable_2_by_2_CPU(self):with context.device(CPU):resource_variable_ops.ResourceVariable(self._m_2_by_2)self._benchmark_transpose(m, self._num_iters_2_by_2)def benchmark_tf_transpose_variable_2_by_2_GPU(self):with context.device(GPU):resource_variable_ops.ResourceVariable(self._m_2_by_2)self._benchmark_transpose(m, self._num_iters_2_by_2)def benchmark_defun_without_signature(self):def func(t1, t2, t3, t4, t5, t6, t7, t8):del t1, t2, t3, t4, t5, t6, t7, t8return Nonefunction.defun(func)constant_op.constant(0.0)lambda: defined(t, t, t, t, t, t, t, t)self._run(cache_computation, 30000)def benchmark_defun_without_signature_and_with_kwargs(self):def func(t1, t2, t3, t4, t5, t6, t7, t8):del t1, t2, t3, t4, t5, t6, t7, t8return Nonefunction.defun(func)constant_op.constant(0.0)def cache_computation():return defined(t, t, t, t, t, t, t, t)self._run(cache_computation, 30000)def benchmark_defun_with_signature(self):def func(t1, t2, t3, t4, t5, t6, t7, t8):del t1, t2, t3, t4, t5, t6, t7, t8return Nonefunction.defun(func, [tensor_spec.TensorSpec([], dtypes.float32)] * 8)constant_op.constant(0.0)lambda: defined(t, t, t, t, t, t, t, t)self._run(signature_computation, 30000)def benchmark_defun_with_signature_and_kwargs(self):def func(t1, t2, t3, t4, t5, t6, t7, t8):del t1, t2, t3, t4, t5, t6, t7, t8return Nonefunction.defun(func, [tensor_spec.TensorSpec([], dtypes.float32)] * 8)constant_op.constant(0.0)def signature_computation():return defined(t, t, t, t, t, t, t, t)self._run(signature_computation, 30000)def benchmark_matmul_read_variable_op_2_by_2_CPU(self):with context.device(CPU):resource_variable_ops.ResourceVariable(self._m_2_by_2)self._benchmark_matmul_read_variable(m, self._num_iters_2_by_2)def benchmark_matmul_read_variable_op_with_tape_2_by_2_CPU(self):with context.device(CPU):resource_variable_ops.ResourceVariable(self._m_2_by_2)self._benchmark_matmul_read_variable_with_tape(m, self._num_iters_2_by_2)def benchmark_read_variable_op_2_by_2_CPU(self):with context.device(CPU):resource_variable_ops.ResourceVariable(self._m_2_by_2)self._benchmark_read_variable(m, self._num_iters_2_by_2)def benchmark_read_variable_op_2_by_2_GPU(self):if not context.num_gpus():returnwith context.device(GPU):resource_variable_ops.ResourceVariable(self._m_2_by_2.gpu())self._benchmark_read_variable(m, self._num_iters_2_by_2)def benchmark_read_variable_op_with_tape_2_by_2_CPU(self):with context.device(CPU):resource_variable_ops.ResourceVariable(self._m_2_by_2)self._benchmark_read_variable_with_tape(m, self._num_iters_2_by_2)def benchmark_read_variable_op_with_tape_2_by_2_GPU(self):if not context.num_gpus():returnwith context.device(GPU):resource_variable_ops.ResourceVariable(self._m_2_by_2.gpu())self._benchmark_read_variable_with_tape(m, self._num_iters_2_by_2)def benchmarkScan(self):math_ops.range(1600)def scan():return functional_ops.scan(lambda a, x: a + x, elems, 1)self._run(scan, 100)@test_util.disable_tfrt("tf.While not supported RTFB tensor. b/169374895")def benchmarkScanDefun(self):math_ops.range(1600)@function.defundef scan():return functional_ops.scan(lambda a, x: a + x, elems, 1)self._run(scan, 100)def benchmark_fastpath_conversion_type_inference(self):constant_op.constant(1., dtypes.float32)def fn():return gen_math_ops.add(c, 1)self._run(fn, 10000)def benchmark_convert_tensor(self):ops.convert_to_tensor(42)def fn():return ops.convert_to_tensor(value)self._run(fn, 10000)def _benchmark_convert_constant(self, value, cached):global GLOBAL_TEST_VALUEvaluedef cached_func():ops.convert_to_tensor(value)def uncached_func():global GLOBAL_TEST_VALUEGLOBAL_TEST_VALUE += 1ops.convert_to_tensor(GLOBAL_TEST_VALUE)cached_func if cached else uncached_funcself._run(func, 10000)def benchmark_convert_python_int(self):self._benchmark_convert_constant(42, True)def benchmark_convert_python_int_uncached(self):self._benchmark_convert_constant(42, False)def benchmark_convert_python_float(self):self._benchmark_convert_constant(42.0, True)def benchmark_convert_python_float_uncached(self):self._benchmark_convert_constant(42.0, False)def benchmark_convert_numpy_int(self):self._benchmark_convert_constant(np.array(42), True)def benchmark_convert_numpy_int_uncached(self):self._benchmark_convert_constant(np.array(42), False)def benchmark_convert_numpy_float(self):self._benchmark_convert_constant(np.array(42.0), True)def benchmark_convert_numpy_float_uncached(self):self._benchmark_convert_constant(np.array(42.0), False)def benchmark_convert_3x_list_to_tensor(self):[1, 2, 3]self._run(lambda: ops.convert_to_tensor(xs), 1000)def benchmark_convert_3x_array_to_tensor(self):np.array([1, 2, 3], np.int32)self._run(lambda: ops.convert_to_tensor(xs), 1000)def benchmark_constant_40x2_list_to_tensor(self):[[0] * 2] * 40self._run(lambda: constant_op.constant(xs), 1000)def benchmark_constant_40x2_array_to_tensor(self):np.array([[0] * 2] * 40, np.int32)self._run(lambda: constant_op.constant(xs), 1000)def benchmark_constant_40x_list_of_2x_arrays_to_tensor(self):[np.array([0] * 2, np.int32)] * 40self._run(lambda: constant_op.constant(xs), 1000)def benchmark_constant_20x20x20_double_list_to_float32_tensor(self):[[[np.linspace(0, 1, 21).tolist()] * 20] * 20]self._run(lambda: constant_op.constant(xs, dtypes.float32), 10000)def benchmark_constant_20x20x20_double_list_to_float64_tensor(self):[[[np.linspace(0, 1, 21).tolist()] * 20] * 20]self._run(lambda: constant_op.constant(xs, dtypes.float64), 10000)def benchmark_list_of_zeros_to_np_array(self):[]for _ in range(1000):values.append(array_ops.zeros((1000,)))self._run(lambda: np.array([x.numpy() for x in values]), 1000)def benchmark_function_trace(self):def func(x):return xself._run(lambda: (def_function.function(func)(x) for x in range(1000)),30000)def _benchmarkFunctionWithResourceInputs(self, num_resources, num_iters):@def_function.functiondef add_all(*args):return math_ops.add_n(*args)with context.device(CPU):[]for _ in range(num_resources):resources.append(resource_variable_ops.ResourceVariable(self._m_2))self._run(lambda: add_all(resources), num_iters)def benchmarkFunctionWithFiveResourceInputs(self):self._benchmarkFunctionWithResourceInputs(5, 1000)def benchmarkFunctionWithFiveHundredResourceInputs(self):self._benchmarkFunctionWithResourceInputs(500, 100)def _benchmarkResourceReadsInCondInInnerFunc(self, var_count):[]for _ in range(var_count):rvars.append(resource_variable_ops.ResourceVariable(1.0))# Note: We want to benchmark the graph building time so we intentionally# add this outer function so that the tf.function gets retraced every time.def benchmark_fn():@def_function.functiondef fn_with_many_reads():@def_function.functiondef fn_with_many_reads_inner():def then_branch():return math_ops.add_n(rvars)def else_branch():return 0.return control_flow_ops.cond(constant_op.constant(True), then_branch, else_branch)return fn_with_many_reads_inner()return fn_with_many_reads()with context.device(CPU):self._run(benchmark_fn, 10)def benchmarkTenThousandResourceReadsInCondInInnerFunc(self):self._benchmarkResourceReadsInCondInInnerFunc(10000)def benchmarkHundredResourceReadsInCondInInnerFunc(self):self._benchmarkResourceReadsInCondInInnerFunc(100)def benchmarkTenResourceReadsInCondInInnerFunc(self):self._benchmarkResourceReadsInCondInInnerFunc(10)def benchmark_tf_name_scope(self):def fn():with ops.name_scope_v2("name"):passself._run(fn, 10000)def benchmark_tf_nest_map_structure(self):{"a": [1, 2, 3], "b": (4, 5, 6)}def fn():nest.map_structure(lambda x: x, nested)self._run(fn, 10000)def benchmark_tf_nest_pack_sequence_as(self):{"a": [1, 2, 3], "b": (4, 5, 6)}nest.flatten(nested)def fn():nest.pack_sequence_as(nested, flat)self._run(fn, 10000)def benchmark_tf_nest_flatten_none(self):def fn():nest.flatten(None)self._run(fn, 100000)def benchmark_tf_nest_flatten(self):{"a": [1, 2, 3], "b": (4, 5, 6)}def fn():nest.flatten(nested)self._run(fn, 100000)def benchmark_tf_nn_convolution_overhead(self):array_ops.ones((1, 1, 1, 1))array_ops.ones((1, 1, 1, 1))def fn():nn_ops.convolution_v2(inputs, filters)self._run(fn, 10000)def benchmark_tf_tensor_shape_creation_overhead(self):# A `TensorShape` is created the first time `EagerTensor.shape` is# called, which puts `TensorShape.__init__` on the hotpath. The# `TensorShape` is created from `EagerTensor._shape_tuple`.array_ops.ones((1, 1))x._shape_tuple()def fn():tensor_shape.TensorShape(shape_tuple)self._run(fn, 100000)if = "__main__":test.main()